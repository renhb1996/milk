# 乳红外模型及数据

### 模型

3.1 牛乳识别分类模型：不同加热温度及时间处理	

3.2.5 参杂样本分类模型：牛乳，羊乳和豆乳混合溶液（先定性）

3.2.6 PLS模型的建立（再定量）

3.3 乳铁蛋白在乳清分离蛋白溶液中的定量分析，PLS模型的建立（乳铁蛋白在乳清分离蛋白溶液中的定量）

3.4 指纹图谱鉴定 PLS-DA模型，SVM模型，PDP模型









### 数据

wholemilk_alldata

#### 数据预处理

检查缺失值和异常值：na.omit()## 移除含有缺失值的观测，可以删除所有含有缺失数据的行

After collecting the FTIR spectra data of samples, the next step is to clean these data and generate proper labels for each sample. In this step, we first remove the data with NA(Not Available) or outliers by R (version
3.4.1) which are abnormal values. As is known to us, spectral dataset used to be of high dimensionality (HD) and the wavenumbers also tend to exhibit high collinearity and correlation,  the principal component analysis (PCA) is applied to characteristic wavelength selection due to its capability in reducing dimensionality and handing multi-collinear and correlated variables.



，按照需求整理数据，PCA选波。（光谱数据是高维的，PCA降维）。（PLS-DA）引用来将pca的故事。

Over the past two decades, the partial least squares-discriminant
analysis (PLS-DA) has gained wide acceptance and huge popularity in
the field of applied research [7–10,34–36], **partly due to its capability in**
**reducing dimensionality and handling multi-collinear and correlated**
**variables** [37–40]. Since spectral dataset used to be of high dimensionality (HD) and the wavenumbers also tend to exhibit high collinearity and correlation, PLS-DA appears to be the most favored classification method. Spectral data coupled with PLS-DA has demonstrated great success in diverse applied domains, i.e. food analysis [34,41] and forensic science [42,43].



**The principal component analysis (PCA)** or partial least squares
(PLS) **data compression methods help to transform the data set,**
**comprising a large number of inter-correlated variables, into a**
**reduced set of new variables**

#### Machine learning prediction

训练集和测试集划分：75：25

抽样方法：分层抽样，无放回

模型构建

调参

确定最优模型

训练

确定最佳建模方法



一、分组

验证，训练集，验证集，测试集，抽样方法，留一法还是分层？

验证集作为调整模型的依据，这样不至于将测试集中的信息泄漏

也就是说我们将数据划分为训练集、验证机和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到最佳的参数，就在测试集上测试一次，测试集上的误差作为泛化误差的近似。

二、验证

公式ACC, PRECOISION,RECALL,F1

三、调参

使用什么核函数，调用那些参数，网格搜索

svm，核函数，tune.svm调参，gamma=0.01,cost=100

rf, 发现mtry取10时err最小，ntree取10000时误差稳定。

knn，网格搜索1：10，挑出准确度最高的tree

lda，无可调参数，先空着不写。



We split the whole dataset into training, testing and validation sets, approximately in the ratio 80%, 10% and 10%, respectively. Specifically, there are 400, 140 and 144 samples for training, validation and testing set, respectively.

Then a shallow CNN model is trained for the food recognition task on
this small database. Given this CNNs model, we classify our collected images into different classes representing candidate labels. Specifically, top n (e.g., 5) predictions from the shallow network are selected as the candidate labels for one image. Finally, we perform manually label  validation to finalize the dataset by eliminating the wrong labelled images.



Then the  classification models developed by k-Nearest Neighbour (kNN), Support Vector Machine (SVM), Random Forest (RF) and Linear Discriminant Analysis (LDA) are trained for the milk heat identification task on this database. Given these model, we classify our collected  samples into different classes representing the degree of heat treatment. 



SVM are inherently two-class classifiers. The way to do multiclass
classification with R package e1701 is to use the ‘one-against-one’-approach, in which k(k−1)/2 binary classifiers are trained and the appropriate class is found by a voting scheme (Meyer, Dimitriadou,
Hornik, Weingessel, & Leisch, 2018). kNN was implemented using the
function “knn” from the R package (Schliep & Hechenbichler, 2016).
The best k was selected using a grid search from k=2 to 10. RF was
implemented with 200 trees using the “Random Forest” function from
the R package (Liaw & Wiener, 2002). LDA was implemented using the
“LDA” function from the “MASS” R package (Venables & Ripley, 2002).



In this paper, we trained and tuned SVM models with R package e1701,  the optimal parameters of gamma and cost  are set as 0.01 and 100, respectively(Meyer, Dimitriadou, Hornik, Weingessel, & Leisch, 2018);   kNN was implemented using the function “kknn” from the R package (Schliep & Hechenbichler, 2016). The k was selected using a grid search from k=2 to 10,  after tuned, the best kernel is rectangular and the best k is 10 for model; RF was implemented using the “Random Forest” function from the R package (Liaw & Wiener, 2002), there is minimum error when we select mtry as 10 and stable error when we set ntree as 10000.LDA was implemented using the “LDA” function from the “MASS” R package (Venables & Ripley, 2002).





All samples were randomly divided into training, validation and testing
sets with a rate of 3:1:1. 

The training subset was then used to develop the classification
models by k-Nearest Neighbour (kNN), Support Vector Machine
(SVM), Random Forest (RF) and Linear Discriminant Analysis (LDA).
The accuracy of models was first evaluated by using the leave-one-out
cross-validation method within the training set. For each classification
approach, a grid search was performed in order to identify the most
optimum parameter by examining the confusion matrix of the predictions
of training set. The optimized models were then used for the
models’ stabilization for each machine learning technique applied. The
previous steps were repeated as part of a 1000 cycle process, where at
each cycle, the training and testing subset samples were randomized
and reshuffled.



SVM are inherently two-class classifiers. The way to do multiclass
classification with R package e1701 is to use the ‘one-against-one’-approach,
in which k(k−1)/2 binary classifiers are trained and the appropriate
class is found by a voting scheme (Meyer, Dimitriadou,
Hornik, Weingessel, & Leisch, 2018). kNN was implemented using the
function “knn” from the R package (Schliep & Hechenbichler, 2016).
The best k was selected using a grid search from k=2 to 10. RF was
implemented with 200 trees using the “Random Forest” function from
the R package (Liaw & Wiener, 2002). LDA was implemented using the
“LDA” function from the “MASS” R package (Venables & Ripley, 2002).